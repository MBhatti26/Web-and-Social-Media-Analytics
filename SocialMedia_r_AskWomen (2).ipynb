{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f15faf-e5a0-4c5e-a083-6cc0b5138388",
        "id": "pruXNzakxEZ9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.4.26)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "\n",
        "REDDIT_CLIENT_ID = \"Ewh28UI9H7w1A4IashZcxg\"\n",
        "REDDIT_SECRET = \"FeNXFO-T4DL3Vwd26SPlDE2tD4XhUA\"\n",
        "REDDIT_USER_AGENT = \"AskWomenScraper v1.0 by u/Signal_Rich7741\"\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=REDDIT_CLIENT_ID,\n",
        "    client_secret=REDDIT_SECRET,\n",
        "    user_agent=REDDIT_USER_AGENT\n",
        ")\n"
      ],
      "metadata": {
        "id": "zCNiHRDqKc8h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "source": [
        "#Auth\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"Ewh28UI9H7w1A4IashZcxg\",  # Replace with your actual client ID\n",
        "    client_secret=\"FeNXFO-T4DL3Vwd26SPlDE2tD4XhUA\",  # Replace with your actual client secret\n",
        "    user_agent=\"AskWomenScraper v1.0 by u/Signal_Rich7741\"  # Replace with your user agent\n",
        ")\n",
        "\n",
        "# Paste my top 20 thread URLs here from the past month:\n",
        "urls = [\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jz543f/women_of_reddit_what_is_the_worst_job_you_ever_had/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jzlyx7/what_article_of_clothingaccessory_makes_you_feel/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jz1r17/how_does_being_blonde_or_brunette_affect_the_way/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jzki9e/what_if_any_safety_precautions_do_you_take_while/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jy0boc/what_did_emotional_abuse_look_like_in_your/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jxgi19/when_did_you_experience_your_first_orgasm/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jvuuhn/what_small_things_tend_to_push_you_over_the_edge/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jvrvjh/what_do_you_wish_you_knew_before_moving_in_with_a/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jsyitz/ladies_whats_the_most_ridiculous_lie_a_man_has/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jqiq6o/women_who_experienced_postpartum_depression_what/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jq3c50/those_who_have_broken_up_because_you_each_wanted/\",\n",
        "  'https://www.reddit.com/r/AskWomen/comments/1k02gmr/ladies_if_you_were_caught_cheating_by_your/',\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jyi6sz/what_helped_get_your_spark_back_after_a_bad_break/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jy0tx3/married_what_is_your_greatest_regret_about/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jst9kk/how_often_do_you_look_at_your_partners_phone/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jpicj1/daughters_whats_something_your_mom_never_told_you/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jpqaxz/women_who_have_completely_changed_careers_in/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jpanfc/how_do_you_view_physical_infidelity_vs_emotional/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jkdgt5/what_is_something_your_mother_raised_you_to/\",\n",
        "  \"https://www.reddit.com/r/AskWomen/comments/1jhitks/how_do_you_initiate_intimacy_with_your_partner/\"\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "rows = []\n",
        "\n",
        "for url in urls:\n",
        "    submission = reddit.submission(url=url)\n",
        "    submission.comments.replace_more(limit=0)  # Expand all comments\n",
        "\n",
        "    for comment in submission.comments.list():\n",
        "        if not comment.author:\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"thread_title\": submission.title,\n",
        "            \"comment_body\": comment.body,\n",
        "            \"comment_author\": comment.author.name,\n",
        "            \"created_utc\": comment.created_utc,\n",
        "            \"comment_depth\": comment.depth,\n",
        "            \"comment_score\": comment.score\n",
        "        })\n",
        "\n",
        "# Save to CSV\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(\"askwomen_manual_top20.csv\", index=False)\n",
        "print(\"✅ Saved CSV with\", len(df), \"comments\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "b4CAkKtqXWar",
        "outputId": "e7c60fd6-4401-4006-e935-fb471b777fbb",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bca69a61e26b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Expand all comments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/praw/models/reddit/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;34m\"\"\"Return the value of ``attribute``.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{self.__class__.__name__!r} object has no attribute {attribute!r}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m         \u001b[0msubmission_listing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomment_listing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_additional_fetch_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPI_PATH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/praw/util/deprecate_args.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 )\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_old_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/praw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             return self._core.request(\n\u001b[0m\u001b[1;32m    964\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"api_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moauth_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         return self._request_with_retries(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mretry_strategy_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         response, saved_exception = self._make_request(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[1;32m    184\u001b[0m     ) -> tuple[Response, None] | tuple[None, Exception]:\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             response = self._rate_limiter.call(\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_header_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/prawcore/rate_limit.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_header_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_set_header_callback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_header_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"refresh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Authorization\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"bearer {self._authorizer.access_token}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/prawcore/auth.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0madditional_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scope\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scopes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrant_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"client_credentials\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0madditional_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/prawcore/auth.py\u001b[0m in \u001b[0;36m_request_token\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authenticator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreddit_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACCESS_TOKEN_PATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mpre_request_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authenticator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Why are these OKAY responses?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/prawcore/auth.py\u001b[0m in \u001b[0;36m_post\u001b[0;34m(self, url, success_status, **data)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess_status\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     ) -> Response:\n\u001b[0;32m---> 51\u001b[0;31m         response = self._requestor.request(\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/prawcore/requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;34m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: BLE001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install packages for preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "!pip install langdetect\n",
        "\n",
        "!pip install contractions\n",
        "\n",
        "import contractions\n",
        "\n",
        "\n",
        "import re\n",
        "from langdetect import detect\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords # Import stopwords\n",
        "stop_words = set(stopwords.words('english')) # Set of stopwords in English\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"askwomen_manual_top20.csv\")\n",
        "\n",
        "# Convert timestamp\n",
        "df[\"comment_datetime\"] = pd.to_datetime(df[\"created_utc\"], unit='s')\n",
        "\n",
        "# Save the new version with the datetime column included\n",
        "df.to_csv(\"askwomen_preprocessed_with_datetime.csv\", index=False)\n",
        "\n",
        "df = pd.read_csv(\"askwomen_preprocessed_with_datetime.csv\")\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "dJA13r3hub8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply Preprocessing\n",
        "\n",
        "# 🧼 Step 1: Remove moderator and deleted/removed comments\n",
        "df = df[\n",
        "    (~df[\"comment_author\"].str.lower().str.contains(\"moderator\", na=False)) &\n",
        "    (~df[\"comment_body\"].str.lower().isin([\"[deleted]\", \"[removed]\"]))\n",
        "].copy()\n",
        "\n",
        "\n",
        "# Step 2: Preprocessing function (preserves emojis, removes Reddit mentions + noise)\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # lowercase\n",
        "    text = contractions.fix(text)  # expand contractions like \"i'm\" → \"i am\"\n",
        "    text = text.strip()  # trim whitespace\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # remove links\n",
        "    text = re.sub(r\"u/\\w+\", \"\", text)  # remove u/username\n",
        "    text = re.sub(r\"r/\\w+\", \"\", text)  # remove r/subreddit\n",
        "    text = re.sub(r\"[!]{2,}\", \"!\", text)  # collapse !!!\n",
        "    text = re.sub(r\"[?]{2,}\", \"?\", text)  # collapse ???\n",
        "    text = re.sub(r\"[.]{2,}\", \".\", text)  # collapse ...\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words) #remove stopwords here\n",
        "\n",
        "    return text\n",
        "\n",
        "df[\"cleaned_comment\"] = df[\"comment_body\"].apply(preprocess)\n",
        "df\n"
      ],
      "metadata": {
        "id": "YUnfRBMT9PH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Install and load spaCy\n",
        "import numpy as np\n",
        "!pip install spacy\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load SpaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ✅ PART 1: Create noun phrase counter for visual insights\n",
        "np_counter = Counter()\n",
        "\n",
        "for comment in df[\"cleaned_comment\"]:\n",
        "    doc = nlp(comment)\n",
        "    for np in doc.noun_chunks:\n",
        "        np_text = np.text.strip().lower()\n",
        "        if len(np_text.split()) >= 2:  # Only meaningful multi-word phrases\n",
        "            np_counter[np_text] += 1\n",
        "\n",
        "# OPTIONAL: View top 20 noun phrases\n",
        "print(np_counter.most_common(20))\n",
        "\n",
        "\n",
        "# ✅ PART 2: Create spaCy-filtered + lemmatized comment column\n",
        "def spacy_filter(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [\n",
        "        token.lemma_ for token in doc\n",
        "        if token.pos_ in {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
        "        and not token.is_stop\n",
        "        and not token.is_punct\n",
        "    ]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Save result into a new column\n",
        "df[\"spacy_filtered\"] = df[\"cleaned_comment\"].apply(spacy_filter)\n",
        "\n",
        "# Preview to confirm\n",
        "df[[\"cleaned_comment\", \"spacy_filtered\"]].head()\n"
      ],
      "metadata": {
        "id": "RhoWV8lDsPHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Filter out very short comments (<10 chars)\n",
        "df = df[df[\"spacy_filtered\"].str.len() >= 10].copy()\n",
        "\n",
        "# Step 5: Detect language, keep all languages for analysis\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "df[\"lang\"] = df[\"spacy_filtered\"].apply(detect_language)\n",
        "\n",
        "df.to_csv(\"askwomen_FINAL_preprocessed.csv\", index=False)\n",
        "df = pd.read_csv(\"askwomen_FINAL_preprocessed.csv\")\n",
        "df\n"
      ],
      "metadata": {
        "id": "CbO776Oc9e_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Analysis"
      ],
      "metadata": {
        "id": "aFG6mwNN4Hod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🟩 Load preprocessed CSV\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"askwomen_FINAL_preprocessed.csv\")\n",
        "\n",
        "# 📏 Add a column for comment length (based on spacy_filtered now!)\n",
        "df[\"len\"] = df[\"spacy_filtered\"].str.len()\n",
        "\n",
        "# 📊 DESCRIPTIVE STATS\n",
        "print(df[\"len\"].describe())         # Summary stats\n",
        "print(df[\"len\"].mode().iloc[0])     # Most common length\n",
        "\n",
        "# 🧑‍💬 AVERAGE COMMENT LENGTH BY AUTHOR (optional)\n",
        "df.groupby(\"comment_author\")[\"len\"].mean().sort_values(ascending=False).head(10)\n",
        "\n",
        "# 🕒 TIME SERIES — COMMENTS PER DAY\n",
        "df[\"comment_datetime\"] = pd.to_datetime(df[\"comment_datetime\"])\n",
        "daily = df.groupby(df[\"comment_datetime\"].dt.date)[\"spacy_filtered\"].count()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,5))\n",
        "daily.plot(kind=\"line\", marker=\"o\")\n",
        "plt.title(\"📅 Comment Frequency Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 🔠 TOP 20 WORDS (from spacy_filtered)\n",
        "from collections import Counter\n",
        "word_counter = Counter()\n",
        "for comment in df[\"spacy_filtered\"]:\n",
        "    word_counter.update(comment.split())\n",
        "\n",
        "top_words = word_counter.most_common(20)\n",
        "\n",
        "# 📊 BARPLOT\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top_words_df = pd.DataFrame(top_words, columns=[\"word\", \"count\"])\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=top_words_df, x=\"count\", y=\"word\", palette=\"flare\")\n",
        "plt.title(\"💬 Top 20 Most Frequent Words (SpaCy Filtered)\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Word\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ☁️ WORD CLOUD (from spacy_filtered)\n",
        "from wordcloud import WordCloud\n",
        "text = \" \".join(df[\"spacy_filtered\"].tolist())\n",
        "wordcloud = WordCloud(width=1000, height=500, background_color='white', colormap='magma', max_words=200).generate(text)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"☁️ Word Cloud of Reddit Comments\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 🌍 LANGUAGE DISTRIBUTION — this part is still okay using cleaned_comment\n",
        "from langdetect import detect\n",
        "def detect_lang(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "df[\"lang\"] = df[\"spacy_filtered\"].apply(detect_lang)\n",
        "\n",
        "lang_counts = df[\"lang\"].value_counts().head(10)\n",
        "plt.figure(figsize=(8,5))\n",
        "lang_counts.plot(kind=\"bar\", color=\"cornflowerblue\")\n",
        "plt.title(\"🗣️ Top Languages Used\")\n",
        "plt.xlabel(\"Language\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 📦 NOUN PHRASES (you already created np_counter earlier using spaCy)\n",
        "top_nps = pd.DataFrame(np_counter.most_common(20), columns=['Noun Phrase', 'Frequency'])\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "top_nps.sort_values(by='Frequency').plot.barh(x='Noun Phrase', y='Frequency', color='mediumslateblue', legend=False)\n",
        "plt.title(\"🔍 Top 20 Most Common Noun Phrases in AskWomen Comments\", fontsize=14)\n",
        "plt.xlabel(\"Frequency\", fontsize=12)\n",
        "plt.ylabel(\"Noun Phrase\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "8Sz8gMO4NL5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UP4MqLcc38B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify Top Threads on Peak Days of Engagement\n",
        "\n",
        "# ✅ Step 1: Convert full datetime to date only\n",
        "df[\"date_only\"] = pd.to_datetime(df[\"comment_datetime\"]).dt.date\n",
        "\n",
        "# ✅ Step 2: Get top 5 most active dates (highest comment volume)\n",
        "top_dates = df[\"date_only\"].value_counts().sort_values(ascending=False).head(5)\n",
        "print(\"📅 Top 5 Most Active Dates:\\n\", top_dates)\n",
        "\n",
        "# ✅ Step 3: Loop through top dates to show most active threads per day\n",
        "for date in top_dates.index:\n",
        "    print(f\"\\n📌 Threads on {date} (Total comments: {top_dates[date]})\")\n",
        "    top_threads = df[df[\"date_only\"] == date][\"thread_title\"].value_counts().head(5)\n",
        "    print(top_threads)"
      ],
      "metadata": {
        "id": "DQ-bW5i9ySrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Extract just the date from the comment datetime\n",
        "df[\"date_only\"] = pd.to_datetime(df[\"comment_datetime\"]).dt.date\n",
        "\n",
        "# Step 2: Identify the top 5 most active dates\n",
        "top_dates = df[\"date_only\"].value_counts().sort_values(ascending=False).head(5).index\n",
        "\n",
        "# Step 3: Filter dataset to only include those dates\n",
        "filtered_df = df[df[\"date_only\"].isin(top_dates)]\n",
        "\n",
        "# Step 4: Group by date and thread title, count comments\n",
        "thread_counts = (\n",
        "    filtered_df.groupby([\"date_only\", \"thread_title\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"comment_count\")\n",
        ")\n",
        "\n",
        "# Optional: Keep only top 5 threads per date for cleaner plot\n",
        "top_threads_per_day = (\n",
        "    thread_counts.groupby(\"date_only\")\n",
        "    .apply(lambda x: x.nlargest(5, \"comment_count\"))\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Step 5: Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    data=top_threads_per_day,\n",
        "    x=\"date_only\",\n",
        "    y=\"comment_count\",\n",
        "    hue=\"thread_title\",\n",
        "    dodge=True,\n",
        "    palette=\"tab10\"\n",
        ")\n",
        "\n",
        "plt.title(\"Top Threads on Most Active Days\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Thread Title\", bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p5k27ALQ2eqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Join all spaCy-filtered comments\n",
        "text = df[\"spacy_filtered\"].dropna().astype(str)\n",
        "\n",
        "# Vectorize using CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=20)\n",
        "X = vectorizer.fit_transform(text)\n",
        "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = df_bow.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"🔗 Correlation Between Top 20 Words in r/AskWomen Comments\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aI6X3P1qbNuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Extract top N frequent words from spaCy-filtered comments\n",
        "text = df[\"spacy_filtered\"].dropna().astype(str)\n",
        "vectorizer = CountVectorizer(max_features=100)\n",
        "X = vectorizer.fit_transform(text)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "# 2. Get sentence embeddings of each word\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "word_embeddings = model.encode(words)\n",
        "\n",
        "# 3. Reduce dimensions using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
        "reduced = tsne.fit_transform(word_embeddings)\n",
        "\n",
        "# 4. Optional: Cluster words using KMeans\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "labels = kmeans.fit_predict(word_embeddings)\n",
        "\n",
        "# 5. Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "palette = sns.color_palette(\"Set2\", n_colors=5)\n",
        "for i, word in enumerate(words):\n",
        "    plt.scatter(reduced[i, 0], reduced[i, 1], color=palette[labels[i]], s=100)\n",
        "    plt.text(reduced[i, 0]+0.5, reduced[i, 1], word, fontsize=9)\n",
        "\n",
        "plt.title(\"🧠 Word Clusters via Semantic Embeddings + t-SNE\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TaatW9D8bVYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bar Chart: Most Active Threads (by Comment Volume)\n",
        "\n",
        "thread_counts = df[\"thread_title\"].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(y=thread_counts.index, x=thread_counts.values, palette=\"viridis\")\n",
        "plt.title(\"🧵 Top 10 Most Commented Threads in AskWomen March-April\")\n",
        "plt.xlabel(\"Number of Comments\")\n",
        "plt.ylabel(\"Thread Title\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vyHJT6WlKeBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Which comments resonated most with readers?\n",
        "\n",
        "#Most upvoted comments top10\n",
        "top_upvoted = df.sort_values(by=\"comment_score\", ascending=False)[[\"cleaned_comment\", \"comment_score\"]].head(10)\n",
        "display(top_upvoted)\n",
        "\n",
        "# Total number of comments\n",
        "total_comments = len(df)\n",
        "\n",
        "# Comments with > 1000 upvotes\n",
        "viral_comments = df[df[\"comment_score\"] > 1000]\n",
        "viral_count = len(viral_comments)\n",
        "\n",
        "# Percentage calculation\n",
        "viral_percentage = (viral_count / total_comments) * 100\n",
        "print(f\"{viral_count} out of {total_comments} comments ({viral_percentage:.2f}%) received over 1,000 upvotes.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DuFfzHkkN1y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📊 Average upvote score per thread\n",
        "df.groupby(\"thread_title\")[\"comment_score\"].mean().sort_values(ascending=False).head(10).plot(\n",
        "    kind='barh', color='orchid', figsize=(10,6), title=\"💡 Avg Upvotes per Thread\"\n",
        ")\n",
        "plt.xlabel(\"Average Upvote Score\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "CWAhCzbFaxUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Group by thread and calculate:\n",
        "# - total comments per thread\n",
        "# - average upvote score per thread (or total, if you prefer)\n",
        "thread_stats = df.groupby(\"thread_title\").agg(\n",
        "    num_comments=(\"cleaned_comment\", \"count\"),\n",
        "    avg_upvotes=(\"comment_score\", \"mean\")\n",
        ").reset_index()\n",
        "\n",
        "# Calculate Pearson correlation\n",
        "correlation = thread_stats[\"num_comments\"].corr(thread_stats[\"avg_upvotes\"])\n",
        "print(f\"Correlation between number of comments and average upvotes: {correlation:.2f}\")\n",
        "\n",
        "# Visualize it\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=thread_stats, x=\"num_comments\", y=\"avg_upvotes\", color=\"orchid\")\n",
        "plt.title(\"📈 Correlation Between Number of Comments and Average Upvotes per Thread\")\n",
        "plt.xlabel(\"Number of Comments per Thread\")\n",
        "plt.ylabel(\"Average Upvotes per Thread\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Also calcuate Pearson correlation coefficient\n",
        "correlation = thread_stats[\"num_comments\"].corr(thread_stats[\"avg_upvotes\"])\n",
        "print(f\"Correlation: {correlation:.2f}\")\n"
      ],
      "metadata": {
        "id": "jHn720-Iw56I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analysis"
      ],
      "metadata": {
        "id": "CLzf9xKJ4q7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our own sentiment word mapping from VADER\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download(\"vader_lexicon\")\n",
        "\n",
        "# Load base VADER lexicon\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "mapping = dict(analyzer.lexicon)  # Convert from defaultdict to dict for easy editing\n",
        "\n",
        "# Preview extreme values\n",
        "{word: score for word, score in mapping.items() if abs(score) > 3}"
      ],
      "metadata": {
        "id": "52kWnVD3hNEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary tokenizer\n",
        "#nltk.download('punkt')  # Only this is needed # This is not needed because we're using punkt_tab\n",
        "\n",
        "# Define the bigram-only sentiment scoring function\n",
        "def custom_sentiment_score_with_bigrams(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Generate bigrams only\n",
        "    bigrams = [' '.join(bigram) for bigram in ngrams(tokens, 2)]\n",
        "\n",
        "    score = 0\n",
        "    count = 0\n",
        "    for gram in bigrams:\n",
        "        if gram in mapping:  # Uses your adjusted VADER lexicon\n",
        "            score += mapping[gram]\n",
        "            count += 1\n",
        "\n",
        "    return round(score / count, 3) if count > 0 else 0\n",
        "\n",
        "# Apply to cleaned comment column\n",
        "df[\"custom_sentiment_bigram\"] = df[\"cleaned_comment\"].apply(custom_sentiment_score_with_bigrams)\n",
        "df"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "h97ZqAwL2u-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Custom sentiment scoring with bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download the 'punkt_tab' data package\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def custom_sentiment_score_with_bigrams(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Create both unigrams and bigrams\n",
        "    all_grams = tokens + [' '.join(bigram) for bigram in ngrams(tokens, 2)]\n",
        "\n",
        "    score = 0\n",
        "    count = 0\n",
        "    for gram in all_grams:\n",
        "        if gram in mapping:\n",
        "            score += mapping[gram]\n",
        "            count += 1\n",
        "\n",
        "    return round(score / count, 3) if count > 0 else 0\n",
        "\n",
        "# Apply to cleaned comments\n",
        "df[\"custom_sentiment_bigram\"] = df[\"cleaned_comment\"].apply(custom_sentiment_score_with_bigrams)\n",
        "df"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9DWa65cbe1w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom scoring based on our mapping\n",
        "def custom_sentiment_score(text):\n",
        "    text = text.lower()\n",
        "    score = 0\n",
        "    n = 0\n",
        "    for word in text.split():\n",
        "        if word in mapping:\n",
        "            score += mapping[word]\n",
        "            n += 1\n",
        "    return round(score / n, 3) if n > 0 else 0\n",
        "\n",
        "df[\"custom_sentiment\"] = df[\"cleaned_comment\"].apply(custom_sentiment_score)\n",
        "df\n"
      ],
      "metadata": {
        "id": "Mi3pdQkFilwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare averages for custom sentiment and custom sentiment with bigrams\n",
        "\n",
        "print(\"Mean Unigram Sentiment:\", round(df[\"custom_sentiment\"].mean(), 3))\n",
        "print(\"Mean Bigram-Inclusive Sentiment:\", round(df[\"custom_sentiment_bigram\"].mean(), 3))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Unigram sentiment\n",
        "plt.hist(df[\"custom_sentiment\"], bins=25, alpha=0.6, label=\"Unigram Sentiment\", color='mediumseagreen', edgecolor='black')\n",
        "\n",
        "# Bigram-inclusive sentiment\n",
        "plt.hist(df[\"custom_sentiment_bigram\"], bins=25, alpha=0.6, label=\"Bigram-Inclusive Sentiment\", color='mediumpurple', edgecolor='black')\n",
        "\n",
        "plt.title(\"📈 Distribution of Custom Sentiment Scores: Unigram vs. Bigram\")\n",
        "plt.xlabel(\"Sentiment Score\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "1d8qZxXv6iAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Top 5 most positive comments\n",
        "print(\"✨ Most Positive Comments:\")\n",
        "print(df.sort_values(\"custom_sentiment\", ascending=False)[[\"comment_body\", \"custom_sentiment\"]].head(5))\n",
        "\n",
        "# Top 5 most negative comments\n",
        "print(\"\\n💔 Most Negative Comments:\")\n",
        "print(df.sort_values(\"custom_sentiment\")[[\"comment_body\", \"custom_sentiment\"]].head(5))\n"
      ],
      "metadata": {
        "id": "Nd3Kku1Klvx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot histogram of custom sentiment\n",
        "df[\"custom_sentiment\"].plot(\n",
        "    kind='hist',\n",
        "    bins=20,\n",
        "    color='mediumseagreen',\n",
        "    edgecolor='black',\n",
        "    title=\"📈 Frequency Distribution of Custom Sentiment Scores\"\n",
        ")\n",
        "plt.xlabel(\"Custom Sentiment Score\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show mean score\n",
        "print(\"🧮 Mean Custom Sentiment Score:\", round(df[\"custom_sentiment\"].mean(), 3))\n"
      ],
      "metadata": {
        "id": "ip5SBTCyjX3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also score sentiment with TextBlob"
      ],
      "metadata": {
        "id": "G6kHVErS8cUh"
      }
    },
    {
      "source": [
        "# Install and import TextBlob\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Define function to calculate TextBlob polarity score\n",
        "def get_textblob_sentiment(text):\n",
        "    return TextBlob(text).sentiment.polarity  # Range: -1 (negative) to +1 (positive)\n",
        "\n",
        "# Apply to 'cleaned_comment' column\n",
        "df['textblob_score'] = df['cleaned_comment'].apply(get_textblob_sentiment)\n",
        "\n",
        "# Plot sentiment score distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df[\"textblob_score\"].plot(\n",
        "    kind='hist',\n",
        "    bins=20,\n",
        "    color='mediumseagreen',\n",
        "    edgecolor='black',\n",
        "    title=\"📈 Frequency Distribution of TextBlob Sentiment Scores\"\n",
        ")\n",
        "plt.xlabel(\"TextBlob Polarity Score\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print mean sentiment score\n",
        "print(\"🧮 Mean TextBlob Sentiment Score:\", round(df[\"textblob_score\"].mean(), 3))\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "C9TklfkkoBp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets compare Custom Sentiment Score and TextBlob"
      ],
      "metadata": {
        "id": "obel1HMl5SBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_df = df[['textblob_score', 'custom_sentiment']]\n",
        "print(comparison_df.describe())\n",
        "\n",
        "comparison_df.plot(kind='box', title=\"📦 Sentiment Score Distribution: TextBlob vs Custom Model\")\n",
        "plt.ylabel(\"Sentiment Score\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "hlCxqjoT1b9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot overlaid histograms\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# TextBlob histogram\n",
        "df[\"textblob_score\"].plot(\n",
        "    kind='hist',\n",
        "    bins=25,\n",
        "    alpha=0.6,\n",
        "    color='mediumseagreen',\n",
        "    edgecolor='black',\n",
        "    label='TextBlob Sentiment'\n",
        ")\n",
        "\n",
        "# Custom sentiment histogram\n",
        "df[\"custom_sentiment\"].plot(\n",
        "    kind='hist',\n",
        "    bins=25,\n",
        "    alpha=0.6,\n",
        "    color='mediumpurple',\n",
        "    edgecolor='black',\n",
        "    label='Custom Sentiment'\n",
        ")\n",
        "\n",
        "# Plot formatting\n",
        "plt.title(\"📊 Overlay: TextBlob vs Custom Sentiment Score Distribution\")\n",
        "plt.xlabel(\"Sentiment Score\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print mean sentiment scores\n",
        "print(\"🧮 Mean TextBlob Sentiment Score:\", round(df[\"textblob_score\"].mean(), 3))\n",
        "print(\"🧮 Mean Custom Sentiment Score:\", round(df[\"custom_sentiment\"].mean(), 3))\n"
      ],
      "metadata": {
        "id": "djm0YywZ_JBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic Modelling LDA"
      ],
      "metadata": {
        "id": "KKXUB52G5VuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "Kv9wqoZA5VcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# Load your dataset - Assuming your CSV is named 'askwomen_FINAL_preprocessed.csv'\n",
        "df = pd.read_csv(\"askwomen_FINAL_preprocessed.csv\")\n",
        "\n",
        "# Tokenized spaCy column for LDA\n",
        "texts = df[\"spacy_filtered\"].dropna().astype(str).str.split()\n",
        "\n",
        "# Filter out stopwords again (if needed)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "texts = [[word for word in doc if word not in stop_words] for doc in texts]\n",
        "\n",
        "# Create dictionary and corpus\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "z7jc05y98b29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train LDA model\n",
        "lda_model = LdaModel(corpus=corpus,\n",
        "                     id2word=dictionary,\n",
        "                     num_topics=6,       # Change if needed\n",
        "                     random_state=42,\n",
        "                     passes=10,\n",
        "                     alpha='auto',\n",
        "                     per_word_topics=True)\n"
      ],
      "metadata": {
        "id": "CDYS6DU2nIIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print top words per topic\n",
        "\n",
        "for i, topic in lda_model.show_topics(num_topics=6, num_words=10, formatted=False):\n",
        "    print(f\"\\n🔹 Topic {i+1}:\")\n",
        "    print(\", \".join([word for word, prob in topic]))\n"
      ],
      "metadata": {
        "id": "CxSpiYfinYt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dictionary to label topics\n",
        "topic_labels = [\n",
        "    \"Reflection & Identity\",       # Topic 1\n",
        "    \"Appearance & Gender\",         # Topic 2\n",
        "    \"Moderation & Platform\",       # Topic 3\n",
        "    \"Relationships & Regret\",      # Topic 4\n",
        "    \"Trust & Technology\",          # Topic 5\n",
        "    \"Work & Life\"                  # Topic 6\n",
        "]\n",
        "#Visualize Topic Distribution\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get dominant topic per document\n",
        "topic_distribution = []\n",
        "for bow in corpus:\n",
        "    topic_probs = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
        "    dominant_topic = max(topic_probs, key=lambda x: x[1])[0]\n",
        "    topic_distribution.append(dominant_topic)\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(topic_distribution, bins=range(7), align='left', color=\"mediumorchid\", edgecolor=\"black\")\n",
        "plt.title(\"🧠 Topic Distribution Across Reddit Comments\")\n",
        "plt.xlabel(\"Topic Number\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.xticks(ticks=np.arange(6), labels=topic_labels, rotation=45, ha='right')\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Xa6w0zLkneha"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}